{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"automated_flair12.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1WHwZVM86mynBJimqtKa-Jst0Ar-5g-LF","authorship_tag":"ABX9TyOSSXqdEMNbS2DFRpskMV2D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CO-vngxhLXu-"},"source":["Install Necessary Modules"]},{"cell_type":"code","metadata":{"id":"v3jYIa1S9n4o"},"source":["!pip3 install flair\r\n","!pip3 install -U pip setuptools wheel\r\n","!pip3 install -U spacy[cuda110]\r\n","!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRkqmLBgTdGl"},"source":["Flair 12 Algorithm"]},{"cell_type":"code","metadata":{"id":"dok_Ad8IThhq"},"source":["import os\n","import sys\n","import logging\n","import json\n","import spacy\n","\n","from flair.models import SequenceTagger\n","from flair.data import Sentence\n","\n","os.chdir(\"/content/drive/MyDrive/USENIX2021/Code/NER\")\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=('[%(asctime)s] %(levelname)-8s %(message)s'),\n","    handlers=[\n","        logging.FileHandler('debug.log'),\n","        logging.StreamHandler(sys.stdout),\n","    ]\n",")\n","\n","flair12class = SequenceTagger.load('ner-ontonotes-fast')\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def getSpaCySentences(text):\n","    sentences = []\n","\n","    doc = nlp(text)\n","\n","    for word in doc:\n","        if word.is_sent_start:\n","            sentences.append([])\n","        if word.text.strip() != '':\n","            sentences[-1].append(word)\n","    return [sentence for sentence in sentences if sentence]\n","\n","def getDocREDVertexSetFromFlairEntities(entities, sentences):\n","    vertexSet = []\n","\n","    entities.sort(key = lambda entity: entity[\"start_pos\"])\n","\n","    entity_id = 0\n","    sent_id = 0\n","    wordIDX = 0\n","    startPos = -1\n","    while sent_id < len(sentences) and entity_id < len(entities):\n","        while wordIDX < len(sentences[sent_id]) and entity_id < len(entities):\n","            if sentences[sent_id][wordIDX].idx + len(sentences[sent_id][wordIDX]) > entities[entity_id][\"start_pos\"] and sentences[sent_id][wordIDX].idx <= entities[entity_id][\"start_pos\"]:\n","                startPos = wordIDX\n","            if sentences[sent_id][wordIDX].idx + len(sentences[sent_id][wordIDX]) >= entities[entity_id][\"end_pos\"]:\n","                '''\n","                if ''.join([word.text for word in sentences[sent_id][startPos:wordIDX + 1]]).find(bratEntities[entity_id][\"name\"].replace(\" \", \"\")) == -1:\n","                    print(\"CATASTROPHIC FAILURE, named entity was not part of sentence\")\n","                    print(''.join([word.text for word in sentences[sent_id][startPos:wordIDX + 1]]))\n","                    print(bratEntities[entity_id][\"name\"].replace(\" \", \"\"))\n","                    input()\n","\n","                if len(sentences[sent_id][startPos:wordIDX + 1]) > 1 and ''.join([word.text for word in sentences[sent_id][startPos + 1:wordIDX + 1]]).find(bratEntities[entity_id][\"name\"].replace(\" \", \"\")) != -1:\n","                    print([word.text for word in sentences[sent_id][startPos:wordIDX + 1]])\n","                    print(bratEntities[entity_id][\"name\"].replace(\" \", \"\"))\n","                    print(\"CATASTROPHIC FAILURE, captured a word before entity appears\")\n","                    input()\n","\n","                if len(sentences[sent_id][startPos:wordIDX + 1]) > 1 and ''.join([word.text for word in sentences[sent_id][startPos:wordIDX]]).find(bratEntities[entity_id][\"name\"].replace(\" \", \"\")) != -1:\n","                    print(\"CATASTROPHIC FAILURE, captured a word after entity appears\")\n","                    input()\n","                '''\n","                vertexSet.append([{\n","                    \"name\": entities[entity_id][\"text\"],\n","                    \"pos\": [startPos, wordIDX + 1],\n","                    \"sent_id\": sent_id,\n","                    \"type\": entities[entity_id][\"labels\"][0].value\n","                }])\n","\n","                entity_id += 1\n","                wordIDX = startPos\n","                startPos = -1\n","                continue\n","            wordIDX += 1\n","        if startPos != -1:\n","            sentences[sent_id].extend(sentences[sent_id + 1])\n","            del sentences[sent_id + 1]\n","        else:\n","            sent_id += 1\n","            wordIDX = 0\n","\n","    return vertexSet\n","\n","def flair12NER(title, text):\n","    s = Sentence(text)\n","    flair12class.predict(s)\n","    entities = s.to_dict(tag_type=\"ner\")\n","    sentences = getSpaCySentences(entities[\"text\"])\n","    vertexSet = getDocREDVertexSetFromFlairEntities(entities[\"entities\"], sentences)\n","    docREDDocumentObject = {\n","        \"vertexSet\": vertexSet,\n","        \"title\": title,\n","        \"sents\": [[word.text for word in sentence] for sentence in sentences]\n","    }\n","    return docREDDocumentObject\n","\n","def main():\n","    allDocREDDocuments = []\n","    for root, _, files in os.walk(\"Threat Reports\"):\n","        for filename in files:\n","            if filename[0] != '.' and filename.lower() != \"readme.txt\" and os.path.splitext(filename)[1] == \".txt\":\n","                try:\n","                    with open(os.path.join(root, filename), \"r\", encoding=\"windows_1252\") as file:\n","                        text = file.read()\n","                except Exception as err:\n","                    logging.warning(\"Unable to open/read file '\" + os.path.join(root, filename) + \"' as windows_1252, attempting ISO-8859-1: \" + str(err))\n","                    try:\n","                        with open(os.path.join(root, filename), \"r\", encoding=\"ISO-8859-1\") as file:\n","                            text = file.read()\n","                    except Exception as err:\n","                        logging.error(\"Unable to open/read file '\" + os.path.join(root, filename) + \"': \" + str(err))\n","                        continue\n","\n","                logging.info(\"Running flair12class algorithm on '\" + filename + \"'...\")\n","                try:\n","                    allDocREDDocuments.append(flair12NER(os.path.splitext(filename)[0], text))\n","                    logging.info(\"Completed!\")\n","                except Exception as err:\n","                    logging.error(\"flair12class failed on '\" + os.path.join(root, filename) + \"': \" + str(err))\n","\n","    with open(\"../Preprocessing/threatreport_flair12_data.json\", 'w') as file:\n","        json.dump(allDocREDDocuments, file)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"execution_count":null,"outputs":[]}]}